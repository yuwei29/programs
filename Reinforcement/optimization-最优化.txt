https://www.studysmarter.us/explanations/math/calculus/optimization-problems/
https://tutorial.math.lamar.edu/Problems/CalcI/Optimization.aspx
https://boffinsportal.com/8-examples-of-optimization-problem-in-real-life/
https://math.libretexts.org/Bookshelves/Calculus/Map%3A_Calculus__Early_Transcendentals_(Stewart)/04%3A_Applications_of_Differentiation/4.07%3A_Optimization_Problems

https://www.zhihu.com/question/530428206/answer/2530994202
猎户座
HITsz研一 | 联邦学习 | 图机器学习 | 迁移学习
263 人赞同了该回答
Hilbert曾经说过，一个数学问题能正确描述出来，那么就成功了80%。对于一个优化问题而言，如果能转化为凸优化问题，那么就成功了90%。 ——中科大《凸优化》授课老师凌青
1 最优化问题分类
最优化（或称数学规划）是一个很大的范畴，它是指找出实数函数  的最大值或最小值。一般最优化问题可写作如下形式[1]：


其中  是  维未知自变量向量，  ，而  ，  ，  是关于自变量的实值函数。集合  是  维空间的一个子集。  被称为目标函数，其它等式、不等式被称为约束条件。

目前最优化问题可以按以下四种方式进行分类：

有约束优化和无约束优化
如我们上面所述需要满足一些等式或者不等式的约束的就是有约束优化，没有则是无约束优化。比如SVM模型的求解就是经典的有约束优化问题，需要用到拉格朗日乘子将其先将其约束转换为目标函数，然后再将其转换为对偶问题进行求解。

连续优化和离散优化
根据优化函数的自变量的定义域  是连续的还是离散的，可以将优化问题分为连续优化和离散优化。其中离散优化主要包括整数规划和关于图、拟阵等的组合优化。

线性和非线性规划
线性规划是指目标函数和约束函数都为线性的优化问题（定义域需要连续）。若  是线性函数，则对任意的  和  有下述等式成立：


而非线性规划是指目标函数和约束至少有一个为非线性的优化问题。线性规划一般在运筹学（如 经济模型等）中有重要运用，而非线性规划在机器学习中有着重要的运用。

凸优化和非凸优化
过去人们认为只有线性规划可以高效求解，而现在有一个普遍的观点认为，优化问题的分水岭不是线性和非线性，而是凸与非凸[2]。凸优化问题是指目标函数和约束函数都是凸函数的问题。若  是凸函数，则对任意的  和  且
，
 有下述不等式成立：


可以看出凸性是较线性更为一般的性质：线性函数需要严格满足等式，而凸函数仅仅需要在  和  取特定值的情况下满足不等式。因此线性规划问题也是凸优化问题，可以将凸优化看成是线性规划的扩展。只要问题可以被转换为凸优化问题，那么就能用内点法等等在多项式时间内求解。凸函数给优化带来很大方便的原因在于其任何一个局部极小点都是全局最优解，便于设计出高效的优化算法。

2 凸和非凸问题的求解算法
接下来我们考虑求解算法。我们这里只考虑无约束连续（数值）优化，而不考虑有约束优化与离散优化。

对大多数无约束数值优化算法而言，凸函数和非凸函数都可以拿来用，如梯度下降法[3]、牛顿法[4]、坐标下降法[5]等（当然，对于用到导数的方法，函数得可以求导）。不过，光滑性和凸性会影响到算法的收敛性质，通常来讲更好的凸性和光滑性质会加速算法的收敛。如梯度下降法在光滑强凸函数下可以达到  的线性收敛速率，在光滑凸函数下则达到  的次线性收敛速率，在光滑非凸情况下则只能达到  的次线性收敛速率。

这里需要专门提一下，对于非凸问题而言（如对神经网络的优化），找到其全局最优解是NP-hard的，我们大多数情况下找到的都是其局部最优解。像我们将前面梯度下降法用于非凸问题，其找到的一阶导数为0的点不一定是全局最优点，可能是鞍点。

像几十年前提出的模拟退火、贝叶斯优化这些可以有效解决低维度非凸优化的问题。然而深度学习中大规模神经网络的参数数目巨大，优化问题的维度很高，传统的算法便不再适用。

在此情况下，学者们进行了如下的有益尝试：

从已有的凸优化算法中，选取更适合神经网络的算法，比如Ada系列算法：带动量的随机梯度下降法[6]、AdaGrad[7]、RMSProp[8]、AdaDelta[9]、Adam[10]。这类算法更新模型时，不只利用当前的梯度，还利用历史上所有的梯度信息，并且自适应地（Adaptively）调整步长。
刻画凸优化算法在非凸问题中相对于局部最优解的收敛性质，并研究神经网络模型局部最优模型和全局最优模型的差异。
如果凸优化算法可能陷入鞍点，改进算法以逃离鞍点，如扰动梯度下降法[11]、逃离鞍点算法[12]等。
设计适用于神经网络的非凸优化算法，如等级优化算法[13]。
至于SLAM中的优化问题究竟是哪一类优化问题，我对SLAM领域不熟就不敢妄议了嘿嘿。


https://www.zhihu.com/question/62877363/answer/1831161592
最优控制(optimal control)与最优化(optimization)有什么区别？
数学外行，搜文章看到optimal control和optimization。从字面上看都有"最优"，不知道这两个领域研究的问题和研究方法有什么不同，两个领域有什么相同的地方。

关注者
334
被浏览
90,843
关注问题​写回答
​邀请回答
​好问题 29
​添加评论
​分享
​
收起 ​
查看全部 14 个回答
云卷云舒
云卷云舒​​
数学话题下的优秀答主
专业
已有 2 人赠与了专业徽章
522 人赞同了该回答
没有什么本质的区别。

有的人说：“不对，一个是静态优化，一个是动态优化。”因为很多人学的最优化都是讨论  中的非线性规划，然后 Optimal Control 一般又是另外开一门课，感觉好像不一样。

其实真没啥区别，从泛函分析的观点看就基本没啥区别。什么静态的动态的，无非就是一个在  中考虑问题，一个是在函数空间，比如  或者  ,  中考虑问题罢了。

其实不论怎么样，优化问题的基本结构就是构造（抽象的） Lagrange Functional :

 。（1）

所有具体的拉格朗日函数/泛函都是这个抽象形式的特例罢了。

只不过 的空间性质实在太好了，比如有界闭集就是紧的，同时对偶空间就是自身，对偶作用表现形式就是内积（一般的 Banach 空间可没有内积，但是可以用“线性泛函的作用”替代，也就是（1）式的尖括号），导致优化问题被大大简化了。

比如 的优化问题

Lagrange函数怎么构造的，那不就是对偶向量作用在约束函数上（有限维就表现为内积）么：

 。配合上所谓的KT条件，求导，欧了。

这里的  就是  ,  就是  。

这就是（1）式的特例！

一般的希尔伯特空间还好，由于有表示定理，我们知道其对偶空间就是自身，从而那个拉格朗日乘子其实就是空间本身的一个元素（一个向量），这与  中是一样的 。

一个简单的无限维优化问题：


约束条件是以泛函的形式给出的，泛函的好处就是值域是  （不考虑复泛函）， 的对偶空间还是  ，对偶作用那就是内积（数乘）。

所以在这里，（1）式其实就是  。同样再配上KT条件就行了。

这里的  就是  , 而  就是  。

然后求变分，欧了。

什么？无限维空间不会写KT条件？其实一样的，那就是互补松弛条件照写！

 ，若  则  。

你看，有啥区别嘛！

没有。

然后最优控制无非就是函数空间上一种特殊的最优化问题罢了，只不过约束条件是微分方程。


你依然从最抽象的 Lagrange形式（也就是（1）式）入手（但是这里需要考虑适当的函数空间）。就可以证明 Pontryagin's maximum principle。

当然，这个证明可不简单，不过切入点就是这样。

__________________________________________________________________________

有人质疑说“最优控制最重要和主流的方法 不是胖加押金那套 而是贝尔曼……”

我想说首先，任何一本正规的最优控制教材Pontryagin's maximum principle都是重点介绍的核心结论之一，不是你觉得不是就不是了。另外Bellman方程本身跟我回答中提到的抽象 Lagrange形式是两套方法，Bellman方程得益于最优控制问题本身具备递归结构，这根本不妨碍抽象Lagrange形式的适用性和概括性。

另外我通篇都没有谈及数值优化的内容，麻烦搞清楚。

编辑于 2021-04-13 16:10


https://www.zhihu.com/question/22686770/answer/618437135
王源​​
算法等 2 个话题下的优秀答主
573 人赞同了该回答
学习一门课程首先要了解学习的目的和意义。我们这里暂且不谈学好了运筹学能够走上人生巅峰，迎娶白富美的事情。这里只谈一小点学了运筹学的人和没有学运筹学的人的细微差别。我认为运筹学影响了我做事的方式。运筹学的基本套路是把实际问题建立模型，然后确定决策变量，其次确定优化目标和约束条件。其实在我们工作中和生活中去处理每一件事情的时候，都是可以通过这种方法来迅速抓住事物的本质，达到很高的做事效率的。而没有学过运筹优化的人，则不会有这样的思维，以至于他们在做事情的时候并往往是不能有一套高效的运行机制。我想这个是学习运筹学给我带来的最大的一个启示。

1，基础课程

老生常谈了，要入门运筹学最起码要有基本的数学知识。基本的要求也不高，微积分，线性代数，外加一门编程语言即可。

微积分和线性代数是所有理工科本科生的基础课程，所以这里也就不再赘述了。至于编程语言掌握一门主流编程语言即可，例如C,C++,Python,Java,Julia，因为最终优化问题的求解都要利用一些solver，都需要一门基础的编程语言做支撑。

如果上面所提的几个数学内容不是很自信的同学可以参考如下的视频课程，

Essence of calculus，作者以拉风的动画，深入的理解带你回顾一下微积分的关键概念。该课程的好处是直观清晰，都是短视频，适合曾经学过微积分但是又比较模糊的同学。

线性代数也是一样的 Essence of linear algebra 以非常直观的角度审视了线性代数的重要概念，直观但不失深刻，配合上高大上的动画效果，让人体会到了大道至简的感觉。一共15个视频，每段视频大约十分钟左右，花费不了多少时间就能看完。



2，中级课程

线性规划(Linear Programming)+凸优化(Convex Optimization)+数值优化（Numerical Optimization)

1 线性规划就是目标和约束条件都是线性的了，其问题形式非常的简单，所以说线性规划是最基本的内容。线性规划肯定也是凸优化的一种，但是线性规划有着一些非常特殊的性质。个人感觉线性规划是基础的基础，往后边整数规划，鲁棒优化，半定规划，锥优化等都离不开线性规划这个基础。学习线性规划需要 几何的直观感受+代数的精确描述+一些计算机的编程实际 三个维度。几何的直观感受可以给我们提供非常好的理解线性规划理论的途径，所以千万不要一上来就一顿代数计算，什么单纯型表进基出基一顿折腾，表上作业法搞得热火朝天的。一定一定要先理解几何的直观意义是什么，然后再用代数把直观感受严密表述出来，这时候一堆枯燥的代数公式就非常好理解了。最后当然需要一点计算机的实践，用solver求解一些问题获得一些应用的感受。

线性规划首推 Dimitris Bertsimas / John N. Tsitsiklis 的 Introduction to Linear Optimization.pdf，这本书是一个比较现代的观念看线性规划了，包括了椭球法，单纯形法，内点法，以前一些拓展内容和future work的方向。当然这本书只有前半段是线性规划内容，所以如果只想学线性规划的话 看完前半段即可，虽然他的后半段也极其精彩。

线性规划的视频课程我推荐 台湾交通大学的公开课，方述诚老师讲的线性规划（很不幸发现这个课程下架了 这里无法给出链接，有需要的后期我可以放网盘链接。方老师每次都从几何的直观出发，然后用代数去严密化直观的东西，具备一定的线性代数知识完全就能理解。



2 世界不总是线性的，优化问题向前一步就是凸优化了，相对非凸优化，凸优化要相对容易一点。这里就绕不开 Stephen Boyd 的 convex optimization 那本书了。这本书是主要分为理论、应用和算法三个层面。其实初学者的困惑有几个，1是书中会用到一些矩阵变换或者不等式放缩的东西。这说明你的数学基础有待提高，2很多同学即使能看懂推导过程也不明白为何要有这些一大堆的理论。这个问题就比较困难一点说明你在优化领域涉及不深，同时对实际应用问题感悟也很少。

其实对于凸优化而言真正的难点是在实际的科研和工程问题中，怎么把你的模型建立成凸优化问题，或者怎么样变换成凸优化问题。这个技巧往往是一门艺术了，没有很一般的方法，具体问题具体看待，好在Stephen Boyd的convex optimization这本书在应用这部分给我们举了很多例子，初学者还一下子get不到这里边的妙处，只有真正做过应用问题的回来再看就很有感受。

Stephen Boyd 的 Convex optimization，与之配套的视频课程也有Convex optimization

如果想深入一点可以看 Introductory Lectures on Convex Programming.pdf



3 数值优化是我个人比较推荐的一个课程，其实数值优化侧重算法一点，而不像前面的线性规划和凸优化需要一大堆的理论框架（虽然这些理论都很美）。数值优化很大一部分就是基于导数的优化方法，这类方法在机器学习里边是最常见应用最广泛的。数值优化的理论分析一般侧重于收敛性和收敛速率，所以它的理论是比较单纯一些的。

数值优化推荐的书自然是 Numerical Optimization.pdf

国内有一个相关的视频课程讲得也不错可供参考：数值优化-复旦大学吴立德

上述三门课程的教材和视频课程资源已经被整理到如下链接中可以获取：

运筹学入门课程之三优化(附教材和视频课程)
​mp.weixin.qq.com/s/Ea6rHHapFwN241s-gq5sIw

3 高级课程

多目标优化、随机优化、鲁棒优化、整数规划，混合整数规划，动态规划，元启发式算法、半正定规划等等高级课程这里就无法一一去点评了，如果具备了之前的基础相信去学这些高级课程会轻松一些。


王源
50 次咨询
5.0
科研、算法优秀回答者
东北大学 系统工程博士
去咨询
编辑于 2022-09-26 23:49・IP 属地中国香港


https://zhuanlan.zhihu.com/p/41799394
深度学习中7种最优化算法的可视化与理解




最优化问题的简洁介绍是什么？
因为有人问我，为什么学习机器学习必须要看最优化的书。我没有想到最合适的解答。 从百度百科抄了一段，但是总觉得没讲到核心问题。 最优化方法（也称做运筹学…显示全部 ​

关注者
1,552
被浏览
222,558
关注问题​写回答
​邀请回答
​好问题 12
​1 条评论
​分享
​
54 个回答
默认排序
王小龙
王小龙​
数学话题下的优秀答主
编辑推荐
1,582 人赞同了该回答
最优化，就是：

1. 构造一个合适的目标函数，使得这个目标函数取到极值的解就是你所要求的东西；

2. 找到一个能让这个目标函数取到极值的解的方法。

下面通过两个例子进行解释。

一、图像去噪

假设你手头有一张照片《沙尘暴下依然坚持工作的摄像师》：


你打算让计算机帮你去个噪，把图像变清晰。你对计算机说：


你看，计算机的回复往往是“你丫能不能说机话！”这是因为计算机是无法进行抽象思维的，它不懂重建、去噪、清晰这些复杂的概念，它唯一会的东西就是加减乘除这样的基本运算，你只能使用正确的计算机语句让它去执行对应的基本运算，因此就需要首先把去噪问题转化成一个数学问题，一个函数的求解问题。比如可以试着这样想，去噪，就是要把图像变平滑，于是你对计算机说：给爷来张图片，无比光滑。计算机的回答是：


真的是好光滑，但是且慢！摄像师哪去了？你是想光滑图像，但是也需要保留图像中的有用细节。这就说明你设计的目标不合理。一个好的去噪的目标，应该兼顾两个要求：与原始图像尽量接近，并且尽量光滑。一个合适的目标可以是：寻找一幅图像A，让下面这个目标函数J最小：

J(A) = (A与原始图像尽量接近) + r * (A尽量平滑)

我们要寻找的，就是能让目标函数J最小的图像A，能够兼顾相似和平滑两个要求。因子r用来权衡二者的重要程度。当r取0时，我们得到的最优目标图像就是原始图像自己！因为我们的目标只是相似，而原始图像自己和自己最像，但是这样就没有任何平滑效果；当r取无穷大的时候，为了让整个目标函数J不至于无穷大，我们必须保证A无比平滑，这样就得到上面那张过度平滑，与原始图像毫不相似的无意义图片。因此，为了兼顾两个目标，我们需要把r的值取得不大不小。

有了一个合适的目标函数，下面就需要构造一种获得它的极小值的算法。在图像去噪领域有一大堆算法：卷积去噪、中值去噪、双边滤波、偏微分方程、小波去噪、随机场去噪......它们的作用或多或少都是相同的——求上面那个混合目标函数的最小值。计算机运算获得的去噪图像是：


从这个成功去噪的例子中我们可以看出：合理的目标函数是最优化第一个需要精心考虑的问题，需要直觉和理性；而如何求解目标函数，则是一个数学算法问题。二者都是数学家们和工程师们大显身手的地方。

二、机器学习

题主困惑机器学习和最优化之间为什么联系紧密，是因为对机器学习这个领域不太了解，实际上研究最优化方法最多的人都在这个领域。机器学习的目的，就是为了让计算机代替人来发现数据之间隐藏的关系。

之所以要使用计算机，是因为数据量太大，远远超过人脑的处理能力。比如我们需要从一堆人脸图片里给每个人标上正确的名字，一幅32像素见方的人脸图像有1024颗像素点，你能想象出一百万张这样的照片和1万个人名字之间的关系是什么样吗。再比如给你1万个患者的DNA序列，每个患者的序列由百万级的碱基对构成，你能找到这些天文数字量级的序列和是否患某种疾病之间的联系吗？

答案是不能！所以研究者退而求其次，建立很多学习模型，这些模型输入是一个样本的数据（头像图片、一个人的DNA序列），输出是样本的标签（人名、是否患病）。模型里有大量可以调整的参数，这些参数通过训练，能够学习到数据和标签之间人类无法直接理解的、复杂的关系。科学家期望当模型训练完成后，再拿来一个样本，喂给这个训练好的机器，它能够吐出一个标签，这个标签恰好就是样本对应的那个正确的标签。

目前人们已经研究出一大堆学习模型：神经网络、支持向量机、AdaBoost、随机森林、隐马尔科夫链、卷积神经网络等等。它们的结构差异很大，但是共同点都是拥有一大堆参数，就等着你喂给它数据供它学习。这些模型的学习也需要一个目标函数：让模型的分类错误率尽量小。为了达到目的，模型的训练往往首先给参数赋上随机初值，然后用各种下降法来寻找能让分类错误率更小的参数设置，梯度下降、牛顿法、共轭梯度法和Levenberg—Marquard法都是常见的方法。

随着研究的深入，问题也越来越多，比如下降法往往只能保证找到目标函数的局部最小值，找不到全局最小值，怎么办呢？答案是不一味下降、也适当爬爬山，说不定能跳出小水沟（局部极小值）找到真正的深井（全局极小值），这种算法叫模拟退火。也可以增大搜索范围，让一群蚂蚁（蚁群算法）或者鸟儿（粒子群算法）一齐搜索，或者让参数巧妙地随机改变（遗传算法）。

那么多模型，到底该选哪个？研究者又发现了一个定理“天下没有免费的午餐”定理，意思是没有一个模型能一直比其他模型好，对于不同类型的数据，必须要通过实验才能发现哪种学习模型更适合。机器学习领域也就成了学界灌水严重的领域之一——换模型、调参数就能发文章哎。

下面说到了调参数，问题又来了，到底是参数多了好还是少了好？参数少了模型太笨学不到数据内的复杂关系，参数多了模型太精明又可能会把数据中的随机噪声当作某种关系进行认真学习（过拟合）。最后大家一致认为，确定模型的复杂度时，要保证模型能力足够强，能够学会数据之间的关系，能力又不能太强，以至于耍小聪明乱学习。这种选择模型的思想被称为奥卡姆剃刀：选择有能力的模型中最简单的那个。此外，训练模型的目标并不是为了使训练样本能够被尽量正确分类，更需要对未知新样本有好的分类效果，这样模型才有实用价值，这种能力被称为泛化能力。除了奥卡姆剃刀原理外，训练时引入随机性的模型比确定的模型（比如BP神经网络）具有更好的泛化能力。

模型的更新也是问题。如果引入了新数据，全部模型都需要重新训练是一笔很大的开销，在线学习模型采用来一个样本学一点的模式，能够不断自我更新；半监督学习利用少量带标签的样本训练一个原始模型，然后利用大量无标签数据再学习。

咱们来看看一些经典的学习模型能做成啥样。首先随便画点散点图，红色和白色是两类不同的数据，分类器需要对整个空间做分割，让平均分类错误率尽量小。你可以先想想如果让你来分要如何划分。


首先是神经网络，使用了6个神经元把空间分成了奇怪的形状：


如果神经元数目变成10个，学到的模式将会十分怪异，说明模型过于复杂了：


下面是支持向量机的分类结果，这是这几十年机器学习最重要的成果之一，它的发明是基于结构最小化准则，通俗地讲就是把目标函数设为：

J=模型分类正确率 + r * 模型复杂度

使得模型能够自动选择分类效果好，并且尽量简单的参数。


接下来是随机树，它把空间划分为一系列矩形区域（叶子），所有的叶子区域由一颗树形结构从根节点不断划分而成，随机的意思是树的生长每次划分第一维还是第二维是随机的：



支持向量机对于小样本数据和非线性结构数据的分类有十分优秀的表现：




在机器学习领域，还有很多重要问题被不断讨论，优秀的模型也不断在涌现。这个领域的开山模型是神经元，由其组成的多层神经网络由于训练速度慢、分类效果不佳，在支持向量机出现后很快就失去了热度。大家卯着劲研究怎么面对训练样本不足的窘境，PCA和核方法大行其道，前者致力于减少数据维数，后者致力于从低维数据中学习高维结构。但是近几年随着卷积神经网络的流行，神经网络又焕发出了第二春，研究者发现只要样本量足够大（百万级甚至亿级样本量），网络参数足够多（百万级参数），加上巧妙的防过拟合技术，利用现代并行计算带来的强大计算能力，神经网络能够学得和人类的判别能力一样好。机器学习领域发展了几十年，似乎又回到了出发的地方。

编辑于 2017-03-25 22:54




王子卓
杉数科技创始人/香港中文大学（深圳）教授
174 人赞同了该回答
优化问题的核心有三部分，决策（Decision），目标（Objective）和约束（Constraint）。






优化的目的是在选取一个（或一些决策），在满足一定约束情况下，尽可能达到某一目标。在去思考优化问题时，最好的顺寻就是问以下问题：


1、我要做的决策是什么？

2、我要达到的目标是什么？

3、我的决策有什么约束？


优化问题的写法也是依照这个顺序：



事实上，所有的决策问题，小到我们生活中的每一个选择，大到国家的战略，都可以分解为这样的三部分（包括所有的机器学习问题）。因此优化的思想可以说是在人们的生活中无处不在，也是世间万物的一种基本规律。数学家欧拉早在18世纪就说过：


Nothing at all takes place in the universe in which some rule of maximum or minimum does not appear. – L. Euler, 1707-1783


在实际中，搞清楚实际问题的这三部分分别是什么，并且用合理的方式去表达是最解决问题中最终要的一步。完成了这一步（所谓的建模）通常已经完成了解决问题的绝大部分。后面就需要用到优化的算法。这两部分也是学习运筹优化的核心。

编辑于 2017-05-28 10:28



运筹之学
40 人赞同了该回答
最优化问题最抽象的说就是

在一定的约束条件下，求一个函数的最大（小）值。


要理解的其实只有两个概念，函数和约束条件。甚至函数这个概念已经包含了对约束条件的考虑。

所谓函数，简单理解的话，可以当做一个机器，你给它一个输入，它就给你一个输出，它是一个对应。你通过调节输入，达到最好的输出。它是现实状况的数学语言表达。例如我们要最小化总费用，我们知道单价，我们可以决定数量，于是我们得到的数学表达：总费用=单价乘以数量。我们通过调整数量来最小的总费用。

至于约束条件，它有很多种，例如等式的约束，不等式的约束，微分方程的约束，概率的约束，等等等等。他们也是对我们现实状况中的约束的数学表达。

不同的约束配上不同的目标函数就会得到一个不同的问题。例如目标函数和约束都是线性的，这个最优化问题就叫线性规划，如果约束是个常微分方程，就叫最优控制。等等等等。

这些问题有的好解，有的不好，所以其实数学建模在这里头的作用很大。对于一个现实问题，建立一个简单好解又能较好地描述现实情况的模型，是一种艺术。这是数学界甚至科学界追求的美的原则：simple and elegant.

编辑于 2015-03-06 08:50



https://zhuanlan.zhihu.com/p/142368160
初中数学重难题型突破系列之——最优方案问题
众望教育
众望教育
67必刷题系列图书官方
​关注
1 人赞同了该文章
近期常常有同学问数姐一些关于最优方案的题，这种题型常常出现在期中期末考试和中考的大题中，问题复杂占分多，一不小心理解错了思路不对就会全盘皆输，往往是很多很多同学的数学扣分点。

冲刺中考刷题必不可少，《初中必刷题》可以随时检测知识漏洞，它独特的八刷体系刷基础刷重点刷中考，锻炼学生做题能力，它题型新颖答案详细，适合于大量、有针对性刷题，提升速度和手感，逐步提高整体学习能力










一般来说，解决方案最优化问题的常见思路是：先根据题目列出相应的函数，再根据函数的性质确定出最优解。这里小编特地找了几个具体题型来和同学们讲解一下具体的思路解法。































看过这几道例题同学们对最优解问题是否有一个更清晰的认知呢？如果大家喜欢重点题型突破这个系列，今后会陆续针对其他疑难点进行讲解归纳，同学们在学习中遇见什么问题也可以随时后台留言。最后祝每个拼搏的同学都能用汗水收获果实。








而这本《教材划重点》教材解读剖析，重点难点一目了然，还有题型整合、章节巩固、高考深度解题等部分，搭配《初中必刷题》使用效果更好，课后先用《教材划重点》复习，然后用《初中必刷题》巩固练习，搭配使用效果翻倍

来源：本文来源于网络，如有侵权，请联系删除

发布于 2020-05-20 18:34


